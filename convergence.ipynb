{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from recast import Model, Weibull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3.0\n",
    "k = 2.0\n",
    "w = Weibull(b, k)\n",
    "inter_times = w.sample((1, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16307251"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_times[0, -10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5129.9805], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrival_times = np.cumsum(inter_times, axis=-1)\n",
    "arrival_times[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = arrival_times[..., -1]\n",
    "seq_lengths = np.sum(arrival_times < t_end, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "class Weibull:\n",
    "    def __init__(self, b, k, eps=1e-8):\n",
    "        self.b = tf.constant(b, dtype=tf.float32)\n",
    "        self.k = tf.constant(k, dtype=tf.float32)\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def prob(self, x):\n",
    "        x = tf.clip_by_value(x, self.eps, tf.reduce_max(x))\n",
    "        return self.b * self.k * tf.math.pow(x, self.k - 1) \\\n",
    "            * tf.math.exp(-self.b * tf.math.pow(x, self.k))\n",
    "\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        x = tf.clip_by_value(x, self.eps, tf.reduce_max(x))\n",
    "        return tf.math.log(self.b) + tf.math.log(self.k) \\\n",
    "            + (self.k - 1) * tf.math.log(x) - self.b * tf.math.pow(x, self.k)\n",
    "    \n",
    "\n",
    "    def log_survival(self, x):\n",
    "        x = tf.clip_by_value(x, self.eps, tf.reduce_max(x))\n",
    "        return -self.b * tf.math.pow(x, self.k)\n",
    "    \n",
    "\n",
    "    def sample(self, size=()):            \n",
    "        u = tf.random.uniform(minval=0,\n",
    "                              maxval=1,\n",
    "                              shape=self.b.shape + size,\n",
    "                              dtype=tf.float32)\n",
    "        y = -1 / self.b * tf.math.log(1 - u)\n",
    "        y = tf.clip_by_value(y, self.eps, tf.reduce_max(y))\n",
    "        return tf.math.pow(y, 1 / self.k).numpy()\n",
    "    \n",
    "\n",
    "class Gamma:\n",
    "    def __init__(self, alpha, beta, eps=1e-8):\n",
    "        self.alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "        self.beta = tf.constant(beta, dtype=tf.float32)\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def prob(self, x):\n",
    "        x = tf.clip_by_value(x, self.eps, tf.reduce_max(x))\n",
    "        return tf.math.pow(x, self.alpha - 1) \\\n",
    "            * tf.math.pow(self.beta, self.alpha) \\\n",
    "            * tf.math.exp(-self.beta * x) \\\n",
    "            / tf.math.exp(tf.math.lgamma(self.alpha))\n",
    "\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        x = tf.clip_by_value(x, self.eps, tf.reduce_max(x))\n",
    "        self.alpha = tf.clip_by_value(self.alpha, self.eps,\n",
    "                                      tf.reduce_max(self.alpha))\n",
    "        self.beta = tf.clip_by_value(self.beta, self.eps,\n",
    "                                     tf.reduce_max(self.beta))\n",
    "        return (self.alpha - 1) * tf.math.log(x) \\\n",
    "            + self.alpha * tf.math.log(self.beta) - self.beta * x \\\n",
    "            - tf.math.lgamma(self.alpha)\n",
    "\n",
    "    \n",
    "    def survival(self, x):\n",
    "        rhs = self.beta * x\n",
    "        rhs = tf.clip_by_value(rhs, self.eps, tf.reduce_max(rhs))\n",
    "        self.alpha = tf.clip_by_value(self.alpha, self.eps,\n",
    "                                      tf.reduce_max(self.alpha))\n",
    "        self.beta = tf.clip_by_value(self.beta, self.eps,\n",
    "                                     tf.reduce_max(self.beta))\n",
    "        return 1 - tf.math.igamma(self.alpha, rhs) \\\n",
    "            / tf.math.exp(tf.math.lgamma(self.alpha))\n",
    "\n",
    "\n",
    "    def log_survival(self, x):\n",
    "        y = self.survival(x)\n",
    "        y = tf.clip_by_value(y, self.eps, tf.reduce_max(y))\n",
    "        return tf.math.log(y)\n",
    "\n",
    "\n",
    "    def sample(self, size=()):            \n",
    "        return tf.random.gamma(size, self.alpha, self.beta).numpy()\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, context_size=32, dist=Weibull):\n",
    "        self.context_size = context_size\n",
    "        self.encoder = keras.layers.GRU(context_size, return_sequences=True)\n",
    "        self.decoder = keras.layers.Dense(2, activation=\"softplus\")\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "        self.dist = dist\n",
    "\n",
    "\n",
    "    def get_context(self, inter_times):\n",
    "        tau = tf.expand_dims(inter_times, axis=-1)\n",
    "        log_tau = tf.math.log(tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "        input = tf.concat([tau, log_tau], axis=-1)\n",
    "        output = self.encoder(input)\n",
    "        context = tf.pad(output[:, :-1, :], [[0, 0], [1, 0], [0, 0]])\n",
    "        return context\n",
    "\n",
    "\n",
    "    def get_inter_times_distribution(self, context):\n",
    "        if self.dist == Weibull or self.dist == Gamma:\n",
    "            params = self.decoder(context)\n",
    "            b = params[..., 0]\n",
    "            k = params[..., 1]\n",
    "            print(b[..., -1], k[..., -1])\n",
    "            return self.dist(b, k)\n",
    "        else:\n",
    "            assert False and \"Distribution not supported\"\n",
    "\n",
    "\n",
    "    def nll_loss(self, inter_times, seq_lengths):\n",
    "        context = self.get_context(inter_times)\n",
    "        inter_times_dist = self.get_inter_times_distribution(context)\n",
    "\n",
    "        log_pdf = inter_times_dist.log_prob(inter_times)\n",
    "        log_surv = inter_times_dist.log_survival(inter_times)\n",
    "\n",
    "        # construit un masque pour ne sélectionner que les éléments\n",
    "        # nécessaires dans chaque liste\n",
    "        mask = np.cumsum(np.ones_like(log_pdf), axis=-1) \\\n",
    "            <= np.expand_dims(seq_lengths, axis=-1)\n",
    "        log_like = tf.reduce_sum(log_pdf * mask, axis=-1)\n",
    "        \n",
    "        # idx est une liste de la forme [(a1, b1), (a2, b2), ...]\n",
    "        # gather_nd sélectionne les éléments correspondant à ces indices\n",
    "        # (ligne et colonne)\n",
    "        idx = list(zip(range(len(seq_lengths)), seq_lengths))\n",
    "        log_surv_last = tf.gather_nd(log_surv, idx)\n",
    "        log_like += log_surv_last\n",
    "\n",
    "        return -log_like\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.encoder.trainable_weights + self.decoder.trainable_weights\n",
    "        \n",
    "    \n",
    "    def fit(self, epochs, inter_times, seq_lengths, t_end):\n",
    "        for epoch in range(epochs + 1):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.reduce_mean(self.nll_loss(inter_times,\n",
    "                                                    seq_lengths)) / t_end\n",
    "            grads = tape.gradient(loss, self.weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.weights))\n",
    "\n",
    "            # if epoch % 10 == 0:\n",
    "            #     print(f\"Loss at epoch {epoch}: {loss:.2f}\")\n",
    "\n",
    "\n",
    "    def sample(self, batch_size, t_end):\n",
    "        inter_times = np.empty((batch_size, 0))\n",
    "        next_context = tf.zeros(shape=(batch_size, 1, 32))\n",
    "        generated = False\n",
    "\n",
    "        while not generated:\n",
    "            dist = self.get_inter_times_distribution(next_context)\n",
    "            next_inter_times = dist.sample()\n",
    "            inter_times = tf.concat([inter_times, next_inter_times], axis=-1)\n",
    "            tau = tf.expand_dims(next_inter_times, axis=-1)\n",
    "            log_tau = tf.math.log(\n",
    "                tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "            input = tf.concat([tau, log_tau], axis=-1)\n",
    "            next_context = self.encoder(input)\n",
    "\n",
    "            generated = np.sum(inter_times, axis=-1).min() >= t_end\n",
    "\n",
    "        return np.cumsum(inter_times, axis=-1)\n",
    "    \n",
    "\n",
    "    def next(self, inter_times, num_preds=1):\n",
    "        for _ in range(num_preds):\n",
    "            next_context = self.get_context(inter_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.8292509], shape=(1,), dtype=float32) tf.Tensor([0.8029413], shape=(1,), dtype=float32)\n",
      "tf.Tensor([1.1166623], shape=(1,), dtype=float32) tf.Tensor([0.92411274], shape=(1,), dtype=float32)\n",
      "tf.Tensor([1.4591743], shape=(1,), dtype=float32) tf.Tensor([1.1377215], shape=(1,), dtype=float32)\n",
      "tf.Tensor([1.984255], shape=(1,), dtype=float32) tf.Tensor([1.453472], shape=(1,), dtype=float32)\n",
      "tf.Tensor([2.7808356], shape=(1,), dtype=float32) tf.Tensor([1.9582082], shape=(1,), dtype=float32)\n",
      "tf.Tensor([3.7577655], shape=(1,), dtype=float32) tf.Tensor([2.6268017], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.293833], shape=(1,), dtype=float32) tf.Tensor([2.7529197], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.445215], shape=(1,), dtype=float32) tf.Tensor([2.579328], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.334197], shape=(1,), dtype=float32) tf.Tensor([2.3216846], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.0738616], shape=(1,), dtype=float32) tf.Tensor([2.112009], shape=(1,), dtype=float32)\n",
      "tf.Tensor([3.75584], shape=(1,), dtype=float32) tf.Tensor([2.0047407], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = Model(context_size=32)\n",
    "model.fit(10, inter_times, seq_lengths, t_end)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "act = keras.activations\n",
    "import pickle\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entra√Ænement avec la loi Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG_PDF~~~~~~ : 1000\n",
      " tf.Tensor(\n",
      "[[        nan -1.1158078  -0.3449545  ...        -inf        -inf\n",
      "         -inf]\n",
      " [        nan -2.2312498  -0.68151593 ...        -inf        -inf\n",
      "         -inf]\n",
      " [        nan -1.0404117  -2.7347007  ...        -inf        -inf\n",
      "         -inf]\n",
      " ...\n",
      " [        nan -2.4676666  -0.5064529  ...        -inf        -inf\n",
      "         -inf]\n",
      " [        nan -1.0413579  -1.6412879  ... -5.9676466         -inf\n",
      "         -inf]\n",
      " [        nan  0.65008783 -0.14993227 ...        -inf        -inf\n",
      "         -inf]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "LOG_SURV~~~~~~ : 0\n",
      " tf.Tensor(\n",
      "[[-0.0000000e+00 -5.6608611e-01 -5.8571255e-01 ... -0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]\n",
      " [-0.0000000e+00 -1.1754100e+00 -4.8082647e-01 ... -0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]\n",
      " [-0.0000000e+00 -9.1536570e-01 -1.4509006e+00 ... -0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]\n",
      " ...\n",
      " [-0.0000000e+00 -1.2951281e+00 -4.4745243e-01 ... -0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]\n",
      " [-0.0000000e+00 -6.8162811e-01 -6.0996848e-01 ... -2.5402618e-11\n",
      "  -0.0000000e+00 -0.0000000e+00]\n",
      " [-0.0000000e+00 -4.1594872e-01 -1.7317243e-01 ... -0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "Loss at epoch 0: nan\n",
      "GRADS~~~~~~ : [192, 3072, 192, 64, 2]\n",
      " [<tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
      "array([[nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>] \n",
      "\n",
      "LOG_PDF~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "LOG_SURV~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "Loss at epoch 1: nan\n",
      "GRADS~~~~~~ : [192, 3072, 192, 64, 2]\n",
      " [<tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
      "array([[nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>] \n",
      "\n",
      "LOG_PDF~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "LOG_SURV~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "Loss at epoch 2: nan\n",
      "GRADS~~~~~~ : [192, 3072, 192, 64, 2]\n",
      " [<tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
      "array([[nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>] \n",
      "\n",
      "LOG_PDF~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "LOG_SURV~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "Loss at epoch 3: nan\n",
      "GRADS~~~~~~ : [192, 3072, 192, 64, 2]\n",
      " [<tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
      "array([[nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>] \n",
      "\n",
      "LOG_PDF~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "LOG_SURV~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "Loss at epoch 4: nan\n",
      "GRADS~~~~~~ : [192, 3072, 192, 64, 2]\n",
      " [<tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2, 96), dtype=float32, numpy=\n",
      "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
      "array([[nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>] \n",
      "\n",
      "LOG_PDF~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n",
      "LOG_SURV~~~~~~ : 135000\n",
      " tf.Tensor(\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]], shape=(1000, 135), dtype=float32) \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6532\\1412655383.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[0minter_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minter_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[0minter_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minter_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minter_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minter_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6532\\1412655383.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, epochs, inter_times, seq_lengths, t_end)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 loss = tf.reduce_mean(self.nll_loss(inter_times,\n\u001b[0;32m     66\u001b[0m                                                     seq_lengths)) / t_end\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;31m# if epoch % 10 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alphonse\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1059\u001b[0m               output_gradients))\n\u001b[0;32m   1060\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1061\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1064\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alphonse\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alphonse\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alphonse\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1361\u001b[0m   \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m   if (isinstance(grad, ops.Tensor) and\n\u001b[0;32m   1363\u001b[0m       \u001b[0m_ShapesFullySpecifiedAndEqual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m       grad.dtype in (dtypes.int32, dtypes.float32)):\n\u001b[1;32m-> 1365\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" vs. \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m   (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) = (\n",
      "\u001b[1;32mc:\\Users\\Alphonse\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7325\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   7326\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7327\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7328\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7329\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7330\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7331\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7332\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self, context_size=32, dist=tfd.Gamma):\n",
    "        self.context_size = context_size\n",
    "        self.encoder = keras.layers.GRU(context_size, return_sequences=True)\n",
    "        self.decoder = keras.layers.Dense(2, activation=\"relu\")\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.dist = dist\n",
    "        self.dist_params = {}\n",
    "\n",
    "\n",
    "    def get_context(self, inter_times):\n",
    "        tau = tf.expand_dims(inter_times, axis=-1)\n",
    "        log_tau = tf.math.log(tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "        input = tf.concat([tau, log_tau], axis=-1)\n",
    "        output = self.encoder(input)\n",
    "        context = tf.pad(output[:, :-1, :], [[0, 0], [1, 0], [0, 0]])\n",
    "        return context\n",
    "\n",
    "\n",
    "    def get_inter_times_distribution(self, context):\n",
    "        params = self.decoder(context)\n",
    "        p1 = params[..., 0]\n",
    "        p2 = params[..., 1]\n",
    "        self.dist_params.setdefault(\"p1\", []).append(p1)\n",
    "        self.dist_params.setdefault(\"p2\", []).append(p2)\n",
    "        # print(\"P1~~~~~~\\n\", p1, \"\\n\")\n",
    "        # print(\"P2~~~~~~\\n\", p2, \"\\n\")\n",
    "        return self.dist(concentration=p1, rate=p2)\n",
    "\n",
    "\n",
    "    def nll_loss(self, inter_times, seq_lengths):\n",
    "        context = self.get_context(inter_times)\n",
    "        inter_times_dist = self.get_inter_times_distribution(context)\n",
    "\n",
    "        log_pdf = inter_times_dist.log_prob(inter_times)\n",
    "        log_surv = inter_times_dist.log_survival_function(inter_times)\n",
    "\n",
    "        # construit un masque pour ne s√©lectionner que les √©l√©ments\n",
    "        # n√©cessaires dans chaque liste\n",
    "        mask = np.cumsum(np.ones_like(log_pdf), axis=-1) \\\n",
    "            <= np.expand_dims(seq_lengths, axis=-1)\n",
    "        log_like = tf.reduce_sum(log_pdf * mask, axis=-1)\n",
    "        \n",
    "        # idx est une liste de la forme [(a1, b1), (a2, b2), ...]\n",
    "        # gather_nd s√©lectionne les √©l√©ments correspondant √† ces indices\n",
    "        # (ligne et colonne)\n",
    "        idx = list(zip(range(len(seq_lengths)), seq_lengths))\n",
    "        log_surv_last = tf.gather_nd(log_surv, idx)\n",
    "        log_like += log_surv_last\n",
    "\n",
    "        print(f\"LOG_PDF~~~~~~ : {np.sum(np.isnan(log_pdf))}\\n\", log_pdf, \"\\n\")\n",
    "        print(f\"LOG_SURV~~~~~~ : {np.sum(np.isnan(log_surv))}\\n\", log_surv, \"\\n\")\n",
    "\n",
    "        return -log_like\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.encoder.trainable_weights + self.decoder.trainable_weights\n",
    "        \n",
    "    \n",
    "    def fit(self, epochs, inter_times, seq_lengths, t_end):\n",
    "        for epoch in range(epochs + 1):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.reduce_mean(self.nll_loss(inter_times,\n",
    "                                                    seq_lengths)) / t_end\n",
    "            grads = tape.gradient(loss, self.weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.weights))\n",
    "\n",
    "            # if epoch % 10 == 0:\n",
    "            #     print(f\"Loss at epoch {epoch}: {loss:.2f}\")\n",
    "\n",
    "            print(f\"Loss at epoch {epoch}: {loss:.2f}\")\n",
    "            sum_nan = [np.sum(np.isnan(grad)) for grad in grads]\n",
    "            print(f\"GRADS~~~~~~ : {sum_nan}\\n\", grads, \"\\n\")\n",
    "\n",
    "\n",
    "    def sample(self, batch_size, t_end):\n",
    "        inter_times = np.empty((batch_size, 0))\n",
    "        next_context = tf.zeros(shape=(batch_size, 1, 32))\n",
    "        generated = False\n",
    "\n",
    "        while not generated:\n",
    "            dist = self.get_inter_times_distribution(next_context)\n",
    "            next_inter_times = dist.sample()\n",
    "            inter_times = tf.concat([inter_times, next_inter_times], axis=-1)\n",
    "            tau = tf.expand_dims(next_inter_times, axis=-1)\n",
    "            log_tau = tf.math.log(\n",
    "                tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "            input = tf.concat([tau, log_tau], axis=-1)\n",
    "            next_context = self.encoder(input)\n",
    "\n",
    "            generated = np.sum(inter_times, axis=-1).min() >= t_end\n",
    "\n",
    "        return np.cumsum(inter_times, axis=-1)\n",
    "    \n",
    "\n",
    "    def next(self, inter_times, num_preds=1):\n",
    "        inter_time = inter_times[-1]\n",
    "        preds = []\n",
    "        for _ in range(num_preds):\n",
    "            last = inter_time\n",
    "            tau = tf.expand_dims(last, axis=-1)\n",
    "            log_tau = tf.math.log(\n",
    "                tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "            input = tf.concat([tau, log_tau], axis=-1)\n",
    "            context = self.encoder(input)\n",
    "            dist = self.get_inter_times_distribution(context)\n",
    "            inter_time = dist.sample(1)\n",
    "            preds.append(inter_time)\n",
    "        return inter_times[-1] + np.cumsum(preds)\n",
    "    \n",
    "file = open(\"data/shchur.pkl\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "t_end = data[\"t_end\"]\n",
    "arrival_times = data[\"arrival_times\"]\n",
    "seq_lengths = [len(times) for times in arrival_times]\n",
    "inter_times_list = [np.diff(times, prepend=0, append=t_end)\n",
    "                    for times in arrival_times]\n",
    "inter_times = np.asarray([np.pad(inter_times, (0, np.max(seq_lengths) - size))\n",
    "        for size, inter_times in zip(seq_lengths, inter_times_list)])\n",
    "inter_times = tf.Variable(inter_times, dtype=tf.float32)\n",
    "inter_times = tf.clip_by_value(inter_times, 1e-8, tf.reduce_max(inter_times))\n",
    "\n",
    "model = Model(context_size=32)\n",
    "model.fit(40, inter_times, seq_lengths, t_end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entra√Ænement bas√© sur un m√©lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 0.91\n",
      "Loss at epoch 1: 0.90\n",
      "Loss at epoch 2: 0.89\n",
      "Loss at epoch 3: 0.88\n",
      "Loss at epoch 4: 0.88\n",
      "Loss at epoch 5: 0.87\n",
      "Loss at epoch 6: 0.87\n",
      "Loss at epoch 7: 0.86\n",
      "Loss at epoch 8: 0.86\n",
      "Loss at epoch 9: 0.85\n",
      "Loss at epoch 10: 0.85\n",
      "Loss at epoch 11: 0.85\n",
      "Loss at epoch 12: 0.84\n",
      "Loss at epoch 13: nan\n",
      "Loss at epoch 14: nan\n",
      "Loss at epoch 15: nan\n",
      "Loss at epoch 16: nan\n",
      "Loss at epoch 17: nan\n",
      "Loss at epoch 18: nan\n",
      "Loss at epoch 19: nan\n",
      "Loss at epoch 20: nan\n",
      "Loss at epoch 21: nan\n",
      "Loss at epoch 22: nan\n",
      "Loss at epoch 23: nan\n",
      "Loss at epoch 24: nan\n",
      "Loss at epoch 25: nan\n",
      "Loss at epoch 26: nan\n",
      "Loss at epoch 27: nan\n",
      "Loss at epoch 28: nan\n",
      "Loss at epoch 29: nan\n",
      "Loss at epoch 30: nan\n"
     ]
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self, context_size=32, R=1, dist=tfd.Weibull):\n",
    "        self.context_size = context_size\n",
    "        self.encoder = keras.layers.GRU(context_size, return_sequences=True)\n",
    "        self.decoder = keras.layers.Dense(R * 3)\n",
    "        self.optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "        self.R = R\n",
    "        self.dist = dist\n",
    "        self.dist_params = {}\n",
    "        self.eps = 1e-8\n",
    "\n",
    "\n",
    "    def get_context(self, inter_times):\n",
    "        tau = tf.expand_dims(inter_times, axis=-1)\n",
    "        log_tau = tf.math.log(tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "        input = tf.concat([tau, log_tau], axis=-1)\n",
    "        output = self.encoder(input)\n",
    "        context = tf.pad(output[:, :-1, :], [[0, 0], [1, 0], [0, 0]])\n",
    "        return context\n",
    "\n",
    "\n",
    "    def get_inter_times_distribution(self, context):\n",
    "        params = self.decoder(context)\n",
    "        w = tfd.Categorical(logits=tf.math.log(act.softmax(params[..., :self.R])))\n",
    "        p1 = act.softplus(params[..., self.R:2*self.R])\n",
    "        p2 = act.softplus(params[..., 2*self.R:])\n",
    "        # print(\"W~~~~~~\\n\", w, \"\\n\")\n",
    "        # print(\"P1~~~~~~\\n\", p1, \"\\n\")\n",
    "        # print(\"P2~~~~~~\\n\", p2, \"\\n\")\n",
    "\n",
    "        self.dist_params.setdefault(\"w\", []).append(w)\n",
    "        self.dist_params.setdefault(\"p1\", []).append(p1)\n",
    "        self.dist_params.setdefault(\"p2\", []).append(p2)\n",
    "        return tfd.MixtureSameFamily(\n",
    "            mixture_distribution=w,\n",
    "            components_distribution=self.dist(p1, p2))\n",
    "\n",
    "\n",
    "    def nll_loss(self, inter_times, seq_lengths):\n",
    "        context = self.get_context(inter_times)\n",
    "        inter_times_dist = self.get_inter_times_distribution(context)\n",
    "\n",
    "        inter_times = tf.clip_by_value(inter_times, self.eps, tf.reduce_max(inter_times))\n",
    "        log_pdf = inter_times_dist.log_prob(inter_times)\n",
    "        log_surv = inter_times_dist.log_survival_function(inter_times)\n",
    "\n",
    "        # construit un masque pour ne s√©lectionner que les √©l√©ments\n",
    "        # n√©cessaires dans chaque liste\n",
    "        mask = np.cumsum(np.ones_like(log_pdf), axis=-1) \\\n",
    "            <= np.expand_dims(seq_lengths, axis=-1)\n",
    "        log_like = tf.reduce_sum(log_pdf * mask, axis=-1)\n",
    "        \n",
    "        # idx est une liste de la forme [(a1, b1), (a2, b2), ...]\n",
    "        # gather_nd s√©lectionne les √©l√©ments correspondant √† ces indices\n",
    "        # (ligne et colonne)\n",
    "        idx = list(zip(range(len(seq_lengths)), seq_lengths))\n",
    "        log_surv_last = tf.gather_nd(log_surv, idx)\n",
    "        log_like += log_surv_last\n",
    "\n",
    "        # print(f\"LOG_PDF~~~~~~ : {np.sum(np.isnan(log_pdf))}\\n\", log_pdf, \"\\n\")\n",
    "        # print(f\"LOG_SURV~~~~~~ : {np.sum(np.isnan(log_surv))}\\n\", log_surv, \"\\n\")\n",
    "\n",
    "        return -log_like\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.encoder.trainable_weights + self.decoder.trainable_weights\n",
    "        \n",
    "    \n",
    "    def fit(self, epochs, inter_times, seq_lengths, t_end):\n",
    "        for epoch in range(epochs + 1):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.reduce_mean(self.nll_loss(inter_times,\n",
    "                                                    seq_lengths)) / t_end\n",
    "            grads = tape.gradient(loss, self.weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.weights))\n",
    "\n",
    "            # if epoch % 10 == 0:\n",
    "            #     print(f\"Loss at epoch {epoch}: {loss:.2f}\")\n",
    "            print(f\"Loss at epoch {epoch}: {loss:.2f}\")\n",
    "            # sum_nan = [np.sum(np.isnan(grad)) for grad in grads]\n",
    "            # print(f\"GRADS~~~~~~ : {sum_nan}\\n\", grads, \"\\n\")\n",
    "            \n",
    "            # break  # debug\n",
    "\n",
    "\n",
    "    def sample(self, batch_size, t_end):\n",
    "        inter_times = np.empty((batch_size, 0))\n",
    "        next_context = tf.zeros(shape=(batch_size, 1, 32))\n",
    "        generated = False\n",
    "\n",
    "        while not generated:\n",
    "            dist = self.get_inter_times_distribution(next_context)\n",
    "            next_inter_times = dist.sample()\n",
    "            inter_times = tf.concat([inter_times, next_inter_times], axis=-1)\n",
    "            tau = tf.expand_dims(next_inter_times, axis=-1)\n",
    "            log_tau = tf.math.log(\n",
    "                tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "            input = tf.concat([tau, log_tau], axis=-1)\n",
    "            next_context = self.encoder(input)\n",
    "\n",
    "            generated = np.sum(inter_times, axis=-1).min() >= t_end\n",
    "\n",
    "        return np.cumsum(inter_times, axis=-1)\n",
    "    \n",
    "\n",
    "    def next(self, inter_times, num_preds=1):\n",
    "        inter_time = inter_times[-1]\n",
    "        preds = []\n",
    "        for _ in range(num_preds):\n",
    "            last = inter_time\n",
    "            tau = tf.expand_dims(last, axis=-1)\n",
    "            log_tau = tf.math.log(\n",
    "                tf.clip_by_value(tau, 1e-8, tf.reduce_max(tau)))\n",
    "            input = tf.concat([tau, log_tau], axis=-1)\n",
    "            context = self.encoder(input)\n",
    "            dist = self.get_inter_times_distribution(context)\n",
    "            inter_time = dist.sample(1)\n",
    "            preds.append(inter_time)\n",
    "        return inter_times[-1] + np.cumsum(preds)\n",
    "\n",
    "\n",
    "file = open(\"data/shchur.pkl\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "t_end = data[\"t_end\"]\n",
    "arrival_times = data[\"arrival_times\"]\n",
    "seq_lengths = [len(times) for times in arrival_times]\n",
    "inter_times_list = [np.diff(times, prepend=0, append=t_end)\n",
    "                    for times in arrival_times]\n",
    "inter_times = np.asarray([np.pad(inter_times, (0, np.max(seq_lengths) - size))\n",
    "        for size, inter_times in zip(seq_lengths, inter_times_list)])\n",
    "inter_times = tf.Variable(inter_times, dtype=tf.float32)\n",
    "inter_times = tf.clip_by_value(inter_times, 1e-8, tf.reduce_max(inter_times))\n",
    "\n",
    "model = Model(context_size=32, R=5, dist=tfd.Weibull)\n",
    "model.fit(30, inter_times, seq_lengths, t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m Model(context_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, R\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, dist\u001b[39m=\u001b[39mtfd\u001b[39m.\u001b[39mWeibull)\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39mfit(\u001b[39m30\u001b[39m, inter_times, seq_lengths, t_end)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model(context_size=32, R=1, dist=tfd.Weibull)\n",
    "model.fit(30, inter_times, seq_lengths, t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.1667\n",
       "1    0.4969\n",
       "2    0.3364\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "prob = tfd.Categorical(probs=[0.1, 0.3, 0.2])\n",
    "samples = prob.sample(10000)\n",
    "pd.Series(samples).value_counts().sort_index() / samples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mtfd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mprobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mforce_probs_to_zero_outside_support\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mallow_nan_stats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Categorical'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Categorical distribution over integers.\n",
      "\n",
      "The Categorical distribution is parameterized by either probabilities or\n",
      "log-probabilities of a set of `K` classes. It is defined over the integers\n",
      "`{0, 1, ..., K-1}`.\n",
      "\n",
      "The Categorical distribution is closely related to the `OneHotCategorical` and\n",
      "`Multinomial` distributions.  The Categorical distribution can be intuited as\n",
      "generating samples according to `argmax{ OneHotCategorical(probs) }` itself\n",
      "being identical to `argmax{ Multinomial(probs, total_count=1) }`.\n",
      "\n",
      "#### Mathematical Details\n",
      "\n",
      "The probability mass function (pmf) is,\n",
      "\n",
      "```none\n",
      "pmf(k; pi) = prod_j pi_j**[k == j]\n",
      "```\n",
      "\n",
      "#### Pitfalls\n",
      "\n",
      "The number of classes, `K`, must not exceed:\n",
      "\n",
      "- the largest integer representable by `self.dtype`, i.e.,\n",
      "  `2**(mantissa_bits+1)` (IEEE 754),\n",
      "- the maximum `Tensor` index, i.e., `2**31-1`.\n",
      "\n",
      "In other words,\n",
      "\n",
      "```python\n",
      "K <= min(2**31-1, {\n",
      "  tf.float16: 2**11,\n",
      "  tf.float32: 2**24,\n",
      "  tf.float64: 2**53 }[param.dtype])\n",
      "```\n",
      "\n",
      "Note: This condition is validated only when `self.validate_args = True`.\n",
      "\n",
      "#### Examples\n",
      "\n",
      "Creates a 3-class distribution with the 2nd class being most likely.\n",
      "\n",
      "```python\n",
      "dist = Categorical(probs=[0.1, 0.5, 0.4])\n",
      "n = 1e4\n",
      "empirical_prob = tf.cast(\n",
      "    tf.histogram_fixed_width(\n",
      "      dist.sample(int(n)),\n",
      "      [0., 2],\n",
      "      nbins=3),\n",
      "    dtype=tf.float32) / n\n",
      "# ==> array([ 0.1005,  0.5037,  0.3958], dtype=float32)\n",
      "```\n",
      "\n",
      "Creates a 3-class distribution with the 2nd class being most likely.\n",
      "Parameterized by [logits](https://en.wikipedia.org/wiki/Logit) rather than\n",
      "probabilities.\n",
      "\n",
      "```python\n",
      "dist = Categorical(logits=np.log([0.1, 0.5, 0.4])\n",
      "n = 1e4\n",
      "empirical_prob = tf.cast(\n",
      "    tf.histogram_fixed_width(\n",
      "      dist.sample(int(n)),\n",
      "      [0., 2],\n",
      "      nbins=3),\n",
      "    dtype=tf.float32) / n\n",
      "# ==> array([0.1045,  0.5047, 0.3908], dtype=float32)\n",
      "```\n",
      "\n",
      "Creates a 3-class distribution with the 3rd class being most likely.\n",
      "The distribution functions can be evaluated on counts.\n",
      "\n",
      "```python\n",
      "# counts is a scalar.\n",
      "p = [0.1, 0.4, 0.5]\n",
      "dist = Categorical(probs=p)\n",
      "dist.prob(0)  # Shape []\n",
      "\n",
      "# p will be broadcast to [[0.1, 0.4, 0.5], [0.1, 0.4, 0.5]] to match counts.\n",
      "counts = [1, 0]\n",
      "dist.prob(counts)  # Shape [2]\n",
      "\n",
      "# p will be broadcast to shape [3, 5, 7, 3] to match counts.\n",
      "counts = [[...]] # Shape [5, 7, 3]\n",
      "dist.prob(counts)  # Shape [5, 7, 3]\n",
      "```\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Initialize Categorical distributions using class log-probabilities.\n",
      "\n",
      "Args:\n",
      "  logits: An N-D `Tensor`, `N >= 1`, representing the unnormalized\n",
      "    log probabilities of a set of Categorical distributions. The first\n",
      "    `N - 1` dimensions index into a batch of independent distributions\n",
      "    and the last dimension represents a vector of logits for each class.\n",
      "    Only one of `logits` or `probs` should be passed in.\n",
      "  probs: An N-D `Tensor`, `N >= 1`, representing the probabilities\n",
      "    of a set of Categorical distributions. The first `N - 1` dimensions\n",
      "    index into a batch of independent distributions and the last dimension\n",
      "    represents a vector of probabilities for each class. Only one of\n",
      "    `logits` or `probs` should be passed in.\n",
      "  dtype: The type of the event samples (default: int32).\n",
      "  force_probs_to_zero_outside_support: Python `bool`. When `True`, negative\n",
      "    values, values greater than N, and non-integer values are evaluated\n",
      "    \"strictly\": `log_prob` returns `-inf`, `prob` returns `0`. When `False`,\n",
      "    the implementation is free to save computation by evaluating something\n",
      "    that matches the Categorical pmf at integer values in the support but\n",
      "    produces an unrestricted result on other inputs.\n",
      "    Default value: `False`.\n",
      "  validate_args: Python `bool`, default `False`. When `True` distribution\n",
      "    parameters are checked for validity despite possibly degrading runtime\n",
      "    performance. When `False` invalid inputs may silently render incorrect\n",
      "    outputs.\n",
      "  allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n",
      "    (e.g., mean, mode, variance) use the value \"`NaN`\" to indicate the\n",
      "    result is undefined. When `False`, an exception is raised if one or\n",
      "    more of the statistic's batch members are undefined.\n",
      "  name: Python `str` name prefixed to Ops created by this class.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\alphonse\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\categorical.py\n",
      "\u001b[1;31mType:\u001b[0m           _AutoCompositeTensorDistributionMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "tfd.Categorical?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
